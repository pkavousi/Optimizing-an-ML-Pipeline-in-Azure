# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run. this project utilizes bankmarketing training data, saved as a CSV file, located at https://automlsamplenotebookdata.blob.core.windows.net/automl-sample-notebook-data/bankmarketing_train.csv.

## Summary
The bankmarketing_train.csv file provided contains personal information about loan applicants, including demographics, employment type, loan application history, and the like. According to the information available on the dataset provided for the project, the data contained in the bankmarketing_train.csv file is related with direct marketing campaigns, via telephonic communications, by a Portuguese banking institution. The classification objective is to predict whether or not the client will subscribe a term deposit (binary, yes/no), so the target column is "y." 

## HyperDrive
The HyperDrive Run was set to maximize accuracy with a maximum of 20 total runs (max 4 concurrent) using the parameter sampling and stopping policies described below. 
The best performing model generated by a sample HyperDriveConfig was a Custom Framework Binary Classification Model using logistic regression with:
Accuracy: 0.9174506828528073
C: 1.1947014936049203
max-iter: 100

The model accuracy was approximately 91.1%.
## Scikit-learn Pipeline
After creating the workspace/environment, and checking for/assigning a compute cluster, the Pipeline began with:
* Importing necessary items from Scikit-Learn and HyperDrive
* Hyperparameter Tuning, including assigning a parameter sampler and selecting parameters/parameter expressions
* Setting a stopping policy for low-performing runs
* Passing parameters to the training script to 
  - Ingest the data (called from csv file)
  - Prepare the data (clean and one hot encode)
  - Split the data into train/test sets
  - Train models
* Use HyperDrive to
  - Generate candidate binary classification models
  - Evaluate models 
  - Return best model based on accuracy

## Parameter Sampler
The HyperDrive Pipeline portion of this project utilized *Random Parameter Sampling*.
The RandomParameterSamplingClass defines random sampling over a hyperparameter search space, and supports early termination of low-performance runs. Hyperparameter values are randomly selected from the defined search space, which allows for a time-effective and cost-effective search for the best hyperparameters.

## Early Stopping Policy
The HyperDrive Pipeline portion of this project utilized *Bandit Policy*.
Bandit Policy uses slack factor (slack amount) and evaluation interval, and terminates the run when the primary metric is not within the specified slack factor compared to the best performing run to that point. With a small slack factor, Bandit Policy has the ability to provide aggressive savings, while still providing solid results. 

## AutoML
The AutoML portion of this project used *Voting Ensemble* to produce the best model.

The VotingEnsemble Class defines an ensemble created from previous AutoML iterations that implements "soft voting," which uses weighted averages. - This method is enabled by default, and appears as one of the final run iterations in an AutoML run. During the VotingEnsemble model generation, multiple fitted models from the previous child runs are downloaded. 

## Pipeline comparison
both Hyperdrive best model and AutoML Voting Ensemble achived almost identical accuracy. Considering the voting ensemble is more computationally expensive, one might go with Hyperdive result.

Since more of the steps are "automated," the AutoML Pipeline structure is simpler, but the layers underneath are even more complicated.
The AutoML Pipeline included direct data ingestion within the Notebook and then:
* Imported the data preparation from the train.py file to
  - Prepare the data (clean and one hot encode)
  - Split the data into train/test sets
* Generated an AutoMLConfig and ran the experiment
  - Within this experiment run 
* Retrieved/Saved the best model 

## Future work
Balancing the data might increase the accuacy of the model. Also, accuracy is not a good metric for imbalanced data. F1 score, or area under Percision-Recall curve is a more realistic metric of models performance and models comparison. Moreover, Bayesian sampling in Hyperdive should be investigated. It could offer a more robust model. 

